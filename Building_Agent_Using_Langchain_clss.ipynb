{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the env variable\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "## Setting the env variable\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TANVILY_API_KEY\")\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY']=GOOGLE_API_KEY\n",
    "os.environ['TAVILY_API_KEY']=TAVILY_API_KEY\n",
    "os.environ['GROQ_API_KEY']=GROQ_API_KEY\n",
    "os.environ['LANGCHAIN_API_KEY']=LANGCHAIN_API_KEY\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT']=LANGCHAIN_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## using the huggingface embedding\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using gemma 2 from chatgroq\n",
    "from langchain_groq import ChatGroq\n",
    "llm_1=ChatGroq(model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "@tool\n",
    "def get_word_length(word:str)->int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_length.invoke(\"ashutosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a:int,b:int)->int:\n",
    "    \"\"\"mutliply two number\"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def summisation(a:int,b:int)->int:\n",
    "    \"\"\"Adding two number\"\"\"\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "mutliply two number\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"a\":10,\"b\":20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summisation\n",
      "Adding two number\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(summisation.name)\n",
    "print(summisation.description)\n",
    "print(summisation.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summisation.invoke({\"a\":10,\"b\":20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Agents\n",
    "- This Agent class from the first version of langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool=load_tools(['wikipedia'],llm=llm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent=initialize_agent(tool,llm_1,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to look up \"Deep Learning\" on Wikipedia. \n",
      "Action: wikipedia\n",
      "Action Input: Deep Learning\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Deep learning\n",
      "Summary: Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\n",
      "Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
      "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\n",
      "\n",
      "Page: Transformer (deep learning architecture)\n",
      "Summary: A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n",
      "Transformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\n",
      "\n",
      "Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: what is Deep Learning?\n",
      "Thought: I need to look up \"Deep Learning\" on Wikipedia. \n",
      "Action: wikipedia\n",
      "Action Input: Deep Learning\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Deep learning\n",
      "Summary: Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\n",
      "Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
      "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\n",
      "\n",
      "Page: Transformer (deep learning architecture)\n",
      "Summary: A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n",
      "Transformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\n",
      "\n",
      "Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: what is Deep Learning?\n",
      "Thought: I need to look up \"Deep Learning\" on Wikipedia. \n",
      "Action: wikipedia\n",
      "Action Input: Deep Learning\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Deep learning\n",
      "Summary: Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\n",
      "Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
      "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\n",
      "\n",
      "Page: Transformer (deep learning architecture)\n",
      "Summary: A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n",
      "Transformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\n",
      "\n",
      "Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDeep learning is a subset of machine learning that uses artificial neural networks with multiple layers to analyze and learn from data. \n",
      "Final Answer: Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to analyze and learn from data. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to analyze and learn from data.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"what is Deep Learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.weatherapi.com/',\n",
       "  'content': \"{'location': {'name': 'Lucknow', 'region': 'Uttar Pradesh', 'country': 'India', 'lat': 26.85, 'lon': 80.9167, 'tz_id': 'Asia/Kolkata', 'localtime_epoch': 1728191684, 'localtime': '2024-10-06 10:44'}, 'current': {'last_updated_epoch': 1728190800, 'last_updated': '2024-10-06 10:30', 'temp_c': 30.2, 'temp_f': 86.4, 'is_day': 1, 'condition': {'text': 'Mist', 'icon': '//cdn.weatherapi.com/weather/64x64/day/143.png', 'code': 1030}, 'wind_mph': 6.3, 'wind_kph': 10.1, 'wind_degree': 125, 'wind_dir': 'SE', 'pressure_mb': 1012.0, 'pressure_in': 29.88, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 70, 'cloud': 0, 'feelslike_c': 32.1, 'feelslike_f': 89.7, 'windchill_c': 31.6, 'windchill_f': 88.9, 'heatindex_c': 34.5, 'heatindex_f': 94.2, 'dewpoint_c': 20.5, 'dewpoint_f': 68.9, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 8.0, 'gust_mph': 7.2, 'gust_kph': 11.6}}\"},\n",
       " {'url': 'https://www.hindustantimes.com/cities/lucknow-news/lucknow-weather-today-aqi-and-rain-forecast-updates-october-6-2024-101728178220022.html',\n",
       "  'content': \"The temperature in Lucknow today, on October 6, 2024, is 30.39 °C. The day's forecast indicates a minimum and maximum temperature of 24.99 °C and 34.44 °C, respectively. The relative humidity ...\"},\n",
       " {'url': 'https://world-weather.info/forecast/india/lucknow/october-2024/',\n",
       "  'content': \"Weather in Lucknow in October 2024 (State of Uttar Pradesh) - Detailed Weather Forecast for a Month Weather Weather in Lucknow Weather in Lucknow in October 2024 Lucknow Weather Forecast for October 2024, is based on previous years' statistical data. +91°+81° +91°+81° +93°+81° +91°+81° +90°+81° +90°+79° +90°+79° +91°+79° +90°+79° +91°+79° +91°+79° +90°+79° +91°+79° +91°+79° +91°+77° +91°+77° +90°+75° +90°+75° +88°+75° +90°+75° +88°+77° +88°+73° +88°+73° +86°+73° +84°+72° +86°+72° +86°+72° +84°+70° +86°+70° +86°+70° +86°+70° Extended weather forecast in Lucknow Weather in large and nearby cities Weather in New Delhi+93° Gwalior+90° Varanasi+90° Rewa+91° Firozabad+90° Satna+90° Mirzāpur+91° Gorakhpur+91° Bareilly+88° Etāwah+90° Allahabad+91° Shāhjahānpur+91° Farrukhābād+90° Kanpur+91° world's temperature today\"},\n",
       " {'url': 'https://www.bbc.com/weather/1264733',\n",
       "  'content': '14-day weather forecast for Lucknow. Homepage. Accessibility links. ... Forecast - Lucknow. Day by day forecast. ... Sunrise 06:00 Sunset 17:50. VH UV Very High.'},\n",
       " {'url': 'https://www.accuweather.com/en/in/lucknow/206678/weather-forecast/206678',\n",
       "  'content': 'Lucknow, Uttar Pradesh, India Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.invoke(\"what is weather of lucknow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "agents=create_tool_calling_agent(llm_1,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agents,tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\pydantic\\main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in lucknow'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'Lucknow', 'region': 'Uttar Pradesh', 'country': 'India', 'lat': 26.85, 'lon': 80.917, 'tz_id': 'Asia/Kolkata', 'localtime_epoch': 1728192273, 'localtime': '2024-10-06 10:54'}, 'current': {'last_updated_epoch': 1728191700, 'last_updated': '2024-10-06 10:45', 'temp_c': 31.3, 'temp_f': 88.3, 'is_day': 1, 'condition': {'text': 'Mist', 'icon': '//cdn.weatherapi.com/weather/64x64/day/143.png', 'code': 1030}, 'wind_mph': 6.0, 'wind_kph': 9.7, 'wind_degree': 128, 'wind_dir': 'SE', 'pressure_mb': 1012.0, 'pressure_in': 29.88, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 66, 'cloud': 0, 'feelslike_c': 33.7, 'feelslike_f': 92.7, 'windchill_c': 31.6, 'windchill_f': 88.9, 'heatindex_c': 34.3, 'heatindex_f': 93.7, 'dewpoint_c': 20.1, 'dewpoint_f': 68.2, 'vis_km': 5.0, 'vis_miles': 3.0, 'uv': 8.0, 'gust_mph': 7.0, 'gust_kph': 11.2}}\"}, {'url': 'https://www.accuweather.com/en/in/lucknow/206678/weather-forecast/206678', 'content': 'Lucknow, Uttar Pradesh, India Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.'}, {'url': 'https://weatherspark.com/h/y/110229/2024/Historical-Weather-during-2024-in-Lucknow-India', 'content': 'Lucknow Temperature History 2024\\nHourly Temperature in 2024 in Lucknow\\nCompare Lucknow to another city:\\nCloud Cover in 2024 in Lucknow\\nObserved Weather in 2024 in Lucknow\\nHours of Daylight and Twilight in 2024 in Lucknow\\nSunrise & Sunset with Twilight in 2024 in Lucknow\\nSolar Elevation and Azimuth in 2024 in Lucknow\\nMoon Rise, Set & Phases in 2024 in Lucknow\\nHumidity Comfort Levels in 2024 in Lucknow\\nWind Speed in 2024 in Lucknow\\nHourly Wind Speed in 2024 in Lucknow\\nHourly Wind Direction in 2024 in Lucknow\\nAtmospheric Pressure in 2024 in Lucknow\\nData Sources\\n While having the tremendous advantages of temporal and spatial completeness, these reconstructions: (1) are based on computer models that may have model-based errors, (2) are coarsely sampled on a 50 km grid and are therefore unable to reconstruct the local variations of many microclimates, and (3) have particular difficulty with the weather in some coastal areas, especially small islands.\\n We further caution that our travel scores are only as good as the data that underpin them, that weather conditions at any given location and time are unpredictable and variable, and that the definition of the scores reflects a particular set of preferences that may not agree with those of any particular reader.\\n See all nearby weather stations\\nLatest Report — 7:30 PM\\nWed, Feb 14, 2024\\xa0\\xa0\\xa0\\xa011 min ago\\xa0\\xa0\\xa0\\xa0UTC 14:00\\nCall Sign VILK\\nTemp.\\n 66°F\\nPrecipitation\\nNo Report\\nWind\\n0.0 mph\\nCloud Cover\\n'}, {'url': 'https://www.accuweather.com/en/in/lucknow/206678/june-weather/206678', 'content': 'Get the monthly weather forecast for Lucknow, Uttar Pradesh, India, including daily high/low, historical averages, to help you plan ahead.'}, {'url': 'https://www.theweathernetwork.com/en/city/in/uttar-pradesh/lucknow/7-days', 'content': 'See the Lucknow, UP, IN extended weather forecast including feels like temperature, wind gust and chance of rain or snow from TheWeatherNetwork.com'}]\u001b[0m\u001b[32;1m\u001b[1;3mThe weather in Lucknow is currently 31.3 degrees Celsius and it is misty. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\pydantic\\main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is weather in lucknow?',\n",
       " 'output': 'The weather in Lucknow is currently 31.3 degrees Celsius and it is misty. \\n'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"what is weather in lucknow?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
